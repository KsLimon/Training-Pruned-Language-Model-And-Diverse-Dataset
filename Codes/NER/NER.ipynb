{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19383,"status":"ok","timestamp":1652568135185,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"},"user_tz":-360},"id":"BbaOlNu__qu6","outputId":"49ae3b60-eced-4d8f-b544-39292d0c8dca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":639,"status":"ok","timestamp":1652568135820,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"},"user_tz":-360},"id":"4uWGAOlJmu__","outputId":"ee4c9aa9-e6d5-456a-e527-caa2c05e0602"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'banglabert'...\n","remote: Enumerating objects: 114, done.\u001b[K\n","remote: Counting objects: 100% (114/114), done.\u001b[K\n","remote: Compressing objects: 100% (89/89), done.\u001b[K\n","remote: Total 114 (delta 52), reused 73 (delta 23), pack-reused 0\u001b[K\n","Receiving objects: 100% (114/114), 1.10 MiB | 6.29 MiB/s, done.\n","Resolving deltas: 100% (52/52), done.\n"]}],"source":["!git clone https://github.com/csebuetnlp/banglabert.git"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1652568136217,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"},"user_tz":-360},"id":"udS7G8Evm1LO","outputId":"7a3f5793-69d3-4313-cd04-71d96d6dc66b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mbanglabert\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}],"source":["ls"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNX0FcdZn-BA","executionInfo":{"status":"ok","timestamp":1652568136217,"user_tz":-360,"elapsed":5,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"}},"outputId":"3edab88f-3dba-4c30-e664-fb5aca05075d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/banglabert\n"]}],"source":["cd banglabert"]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QZVYmiUpEQi","executionInfo":{"status":"ok","timestamp":1652568136217,"user_tz":-360,"elapsed":5,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"}},"outputId":"9a523266-122b-4503-cf36-6bcfebb317e6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mfigs\u001b[0m/                requirements.txt          \u001b[01;34mtoken_classification\u001b[0m/\n","\u001b[01;34mquestion_answering\u001b[0m/  \u001b[01;34msequence_classification\u001b[0m/\n","README.md            setup.sh\n"]}]},{"cell_type":"code","source":["!pip3 install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NkcxFLIgpKWj","executionInfo":{"status":"ok","timestamp":1652568146660,"user_tz":-360,"elapsed":10445,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"}},"outputId":"0360eb66-9a5f-4bf1-b6a8-8471356926b2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/csebuetnlp/normalizer (from -r requirements.txt (line 5))\n","  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-j33sno_q\n","  Running command git clone -q https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-j33sno_q\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 9.6 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (3.17.3)\n","Collecting datasets==1.11.0\n","  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 65.3 MB/s \n","\u001b[?25hCollecting seqeval==1.2.2\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from normalizer==0.0.1->-r requirements.txt (line 5)) (2019.12.20)\n","Collecting emoji==1.4.2\n","  Downloading emoji-1.4.2.tar.gz (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 61.0 MB/s \n","\u001b[?25hCollecting ftfy==6.0.3\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 3.5 MB/s \n","\u001b[?25hCollecting fsspec>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 56.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (21.3)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (6.0.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (2.23.0)\n","Collecting huggingface-hub<0.1.0\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (1.3.5)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (4.11.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (1.21.6)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 3)) (0.70.12.2)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 66.7 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2->-r requirements.txt (line 4)) (1.0.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==6.0.3->normalizer==0.0.1->-r requirements.txt (line 5)) (0.2.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r requirements.txt (line 3)) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r requirements.txt (line 3)) (3.6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0->-r requirements.txt (line 3)) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.11.0->-r requirements.txt (line 3)) (3.0.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0->-r requirements.txt (line 3)) (2021.10.8)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 4)) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 4)) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2->-r requirements.txt (line 4)) (1.4.1)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.11.0->-r requirements.txt (line 3)) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 3)) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 3)) (2.8.2)\n","Building wheels for collected packages: normalizer, seqeval, emoji, ftfy\n","  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6885 sha256=04150028461cd438a4cb85670bbf78de9f4f66202e7d672f9c9a7f0ad46018a7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bhpw1iri/wheels/af/b1/ee/b9e2a2f2dd861976a357b6a6fa105aeedf2254016676f6cf8f\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=162465fcb7957d883250ce917d2a2867f89ad8b9253449bde27977444c9f791a\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=59fc2d9b5519515b0bda70a8702da5753af496915740f4fb120fe4d5caf63d50\n","  Stored in directory: /root/.cache/pip/wheels/e4/61/e7/2fc1ac8f306848fc66c6c013ab511f0a39ef4b1825b11363b2\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=97cf812ec8da490c05b38aa06c3f0a9a0f202cbf2ac3a597afaaa4757f9390d2\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","Successfully built normalizer seqeval emoji ftfy\n","Installing collected packages: xxhash, huggingface-hub, ftfy, fsspec, emoji, seqeval, sentencepiece, normalizer, datasets\n","Successfully installed datasets-1.11.0 emoji-1.4.2 fsspec-2022.3.0 ftfy-6.0.3 huggingface-hub-0.0.19 normalizer-0.0.1 sentencepiece-0.1.96 seqeval-1.2.2 xxhash-3.0.0\n"]}]},{"cell_type":"code","source":["cd token_classification"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjD1Bs2GpUX9","executionInfo":{"status":"ok","timestamp":1652568146661,"user_tz":-360,"elapsed":9,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"}},"outputId":"680b2b9c-a2bb-4edc-c6ed-72c1aa84a21b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/banglabert/token_classification\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"id":"kpIvUGBbrMfs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652568146661,"user_tz":-360,"elapsed":6,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"}},"outputId":"62ada7c0-b7c8-48bd-e5be-c5a74288f154"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["evaluate.sh  README.md  \u001b[0m\u001b[01;34msample_inputs\u001b[0m/  token_classification.py  trainer.sh\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRszJ7kEylGI","executionInfo":{"status":"ok","timestamp":1652568154861,"user_tz":-360,"elapsed":8204,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"}},"outputId":"9edbdc01-5ea9-436c-94b7-c22a2b06fba2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 7.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 52.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 52.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.0.19\n","    Uninstalling huggingface-hub-0.0.19:\n","      Successfully uninstalled huggingface-hub-0.0.19\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datasets 1.11.0 requires huggingface-hub<0.1.0, but you have huggingface-hub 0.6.0 which is incompatible.\u001b[0m\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"]}]},{"cell_type":"code","source":["!python token_classification.py --model_name_or_path \"/content/drive/MyDrive/Models/pruned_bnbert/\" --dataset_dir \"/content/drive/MyDrive/Datasets/NER/iobtag/\" --output_dir \"outputs/\" --learning_rate=2e-5 --warmup_ratio 0.1 --gradient_accumulation_steps 2 --weight_decay 0.1 --lr_scheduler_type \"linear\" --per_device_train_batch_size=16 --per_device_eval_batch_size=16 --max_seq_length 512 --logging_strategy \"epoch\" --save_strategy \"epoch\" --evaluation_strategy \"epoch\" --num_train_epochs=10  --do_train --do_eval"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpEwHZQLv44b","executionInfo":{"status":"ok","timestamp":1652569075718,"user_tz":-360,"elapsed":789262,"user":{"displayName":"Md. Kamrus Samad 1813059642","userId":"16959572353670430778"}},"outputId":"6bdaafd0-61e1-45b4-e112-08810daa7150"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["05/14/2022 22:44:51 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","05/14/2022 22:44:51 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.EPOCH,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","full_determinism=False,\n","gradient_accumulation_steps=2,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=outputs/runs/May14_22-44-50_b3f4f2479e4e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.EPOCH,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=10.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=outputs/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=16,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=outputs/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.EPOCH,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.1,\n","warmup_steps=0,\n","weight_decay=0.1,\n","xpu_backend=None,\n",")\n","05/14/2022 22:44:51 - WARNING - datasets.builder - Using custom data configuration default-213ea30a4054715b\n","05/14/2022 22:44:51 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0)\n","Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0...\n","\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 3639.84it/s]\n","05/14/2022 22:44:51 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","05/14/2022 22:44:52 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","100% 3/3 [00:00<00:00, 112.13it/s]\n","05/14/2022 22:44:52 - INFO - datasets.builder - Generating split train\n","05/14/2022 22:44:52 - INFO - datasets.builder - Generating split validation\n","05/14/2022 22:44:52 - INFO - datasets.builder - Generating split test\n","Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0. Subsequent calls will reuse this data.\n","100% 3/3 [00:00<00:00, 371.36it/s]\n","[INFO|configuration_utils.py:657] 2022-05-14 22:44:53,073 >> loading configuration file /content/drive/MyDrive/Models/pruned_bnbert/config.json\n","[WARNING|configuration_utils.py:310] 2022-05-14 22:44:53,074 >> You passed along `num_labels=2` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1', '2': 'LABEL_2'}. The number of labels wil be overwritten to 3.\n","[INFO|configuration_utils.py:708] 2022-05-14 22:44:53,074 >> Model config BertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Models/pruned_bnbert/\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"bos_token_id\": 101,\n","  \"classifier_dropout\": null,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 102,\n","  \"eos_token_ids\": null,\n","  \"finetuning_task\": \"asl\",\n","  \"gap_size\": 0,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 312,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 1248,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 4,\n","  \"num_memory_blocks\": 0,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertTokenizerFast\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|tokenization_utils_base.py:1702] 2022-05-14 22:44:54,172 >> Didn't find file /content/drive/MyDrive/Models/pruned_bnbert/tokenizer.json. We won't load it.\n","[INFO|tokenization_utils_base.py:1702] 2022-05-14 22:44:54,173 >> Didn't find file /content/drive/MyDrive/Models/pruned_bnbert/added_tokens.json. We won't load it.\n","[INFO|tokenization_utils_base.py:1780] 2022-05-14 22:44:54,174 >> loading file /content/drive/MyDrive/Models/pruned_bnbert/vocab.txt\n","[INFO|tokenization_utils_base.py:1780] 2022-05-14 22:44:54,174 >> loading file None\n","[INFO|tokenization_utils_base.py:1780] 2022-05-14 22:44:54,174 >> loading file None\n","[INFO|tokenization_utils_base.py:1780] 2022-05-14 22:44:54,174 >> loading file /content/drive/MyDrive/Models/pruned_bnbert/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1780] 2022-05-14 22:44:54,174 >> loading file /content/drive/MyDrive/Models/pruned_bnbert/tokenizer_config.json\n","[INFO|modeling_utils.py:1951] 2022-05-14 22:44:54,892 >> loading weights file /content/drive/MyDrive/Models/pruned_bnbert/pytorch_model.bin\n","[WARNING|modeling_utils.py:2255] 2022-05-14 22:44:56,871 >> Some weights of the model checkpoint at /content/drive/MyDrive/Models/pruned_bnbert/ were not used when initializing BertForTokenClassification: ['bert.encoder.layer.1.attention.self.query.weight_mask', 'bert.encoder.layer.0.attention.self.key.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_orig', 'cls.predictions.decoder.weight', 'bert.encoder.layer.1.intermediate.dense.weight_orig', 'bert.encoder.layer.1.attention.self.value.weight_mask', 'bert.encoder.layer.1.attention.self.value.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_mask', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight_orig', 'bert.encoder.layer.1.attention.self.key.weight_mask', 'bert.encoder.layer.0.attention.self.query.weight_mask', 'bert.encoder.layer.0.output.dense.weight_orig', 'bert.encoder.layer.0.attention.self.query.weight_orig', 'bert.encoder.layer.0.attention.self.value.weight_orig', 'cls.predictions.bias', 'bert.encoder.layer.1.attention.self.key.weight_orig', 'bert.encoder.layer.0.attention.output.dense.weight_orig', 'bert.encoder.layer.1.intermediate.dense.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_mask', 'bert.encoder.layer.0.output.dense.weight_mask', 'bert.encoder.layer.0.intermediate.dense.weight_orig', 'bert.encoder.layer.0.attention.self.key.weight_mask', 'bert.encoder.layer.1.attention.self.query.weight_orig', 'bert.encoder.layer.1.attention.output.dense.weight_mask', 'bert.encoder.layer.1.output.dense.weight_mask', 'bert.encoder.layer.0.attention.self.value.weight_mask', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2266] 2022-05-14 22:44:56,871 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Models/pruned_bnbert/ and are newly initialized: ['bert.encoder.layer.1.output.dense.weight', 'classifier.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'classifier.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running normalization on dataset:   0% 0/64155 [00:00<?, ?ex/s]05/14/2022 22:44:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0/cache-771084af922a3430.arrow\n","Running normalization on dataset: 100% 64155/64155 [00:41<00:00, 1563.16ex/s]\n","Running normalization on dataset:   0% 0/3565 [00:00<?, ?ex/s]05/14/2022 22:45:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0/cache-f4b5e5e1e5ebf2eb.arrow\n","Running normalization on dataset: 100% 3565/3565 [00:02<00:00, 1552.40ex/s]\n","Running normalization on dataset:   0% 0/3564 [00:00<?, ?ex/s]05/14/2022 22:45:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0/cache-9c81b7f5d831dbbb.arrow\n","Running normalization on dataset: 100% 3564/3564 [00:02<00:00, 1567.43ex/s]\n","Running tokenizer on dataset:   0% 0/65 [00:00<?, ?ba/s]05/14/2022 22:45:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0/cache-3a09f76ee111c818.arrow\n","Running tokenizer on dataset: 100% 65/65 [00:10<00:00,  6.50ba/s]\n","Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]05/14/2022 22:45:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0/cache-68471a4509007c68.arrow\n","Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  6.14ba/s]\n","Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]05/14/2022 22:45:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-213ea30a4054715b/0.0.0/cache-4d0c29399d0cf30e.arrow\n","Running tokenizer on dataset: 100% 4/4 [00:00<00:00,  7.78ba/s]\n","05/14/2022 22:45:53 - INFO - __main__ - Sample 41905 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 498, 1344, 198, 98, 6965, 2695, 518, 8810, 16330, 4069, 694, 3], 'labels': [-100, 3, 8, -100, 8, 8, 8, 8, 8, 8, 8, 8, -100], 'tags': ['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['তিনি', 'পান্ট', 'এ', 'অভিযানের', 'প্রস্তুতি', 'এবং', 'তহবিল', 'গঠনে', 'সহায়তা', 'করেন']}.\n","05/14/2022 22:45:53 - INFO - __main__ - Sample 7296 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 4798, 3551, 3622, 841, 147, 5269, 9047, 1288, 6123, 863, 2666, 4448, 21034, 3], 'labels': [-100, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, -100], 'tags': ['O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['ফায়ার', 'সার্ভিস', 'কর্মীরা', 'প্রায়', '৬', 'ঘণ্টার', 'চেষ্টায়', 'সকাল', '১০টার', 'দিকে', 'আগুন', 'নিয়ন্ত্রণে', 'আনেন']}.\n","05/14/2022 22:45:53 - INFO - __main__ - Sample 1639 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 750, 3267, 15244, 5362, 26367, 1666, 100, 13168, 12903, 3292, 672, 2616, 341, 2525, 1376, 754, 754, 3197, 1368, 19791, 344, 1758, 2109, 633, 3], 'labels': [-100, 3, 8, 8, -100, -100, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 3, 8, -100, -100, -100, 8, 8, -100], 'tags': ['B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O'], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['তারা', 'বলছেন', 'ফিজিওথেরাপি', 'শিক্ষা', 'ও', 'পেশা', 'নিয়ন্ত্রণের', 'লক্ষ্যে', 'কোনো', 'কাউন্সিল', 'না', 'থাকায়', 'প্রতিদিন', 'হাজার', 'হাজার', 'রোগী', 'অপচিকিত্সার', 'শিকার', 'হচ্ছে']}.\n","[INFO|trainer.py:623] 2022-05-14 22:46:06,618 >> The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1419] 2022-05-14 22:46:06,623 >> ***** Running training *****\n","[INFO|trainer.py:1420] 2022-05-14 22:46:06,623 >>   Num examples = 64155\n","[INFO|trainer.py:1421] 2022-05-14 22:46:06,624 >>   Num Epochs = 10\n","[INFO|trainer.py:1422] 2022-05-14 22:46:06,624 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1423] 2022-05-14 22:46:06,624 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1424] 2022-05-14 22:46:06,624 >>   Gradient Accumulation steps = 2\n","[INFO|trainer.py:1425] 2022-05-14 22:46:06,624 >>   Total optimization steps = 20050\n","{'loss': 0.7014, 'learning_rate': 2e-05, 'epoch': 1.0}\n"," 10% 2005/20050 [01:07<10:03, 29.92it/s][INFO|trainer.py:623] 2022-05-14 22:47:13,721 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:47:13,723 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:47:13,723 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:47:13,723 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  8% 17/223 [00:00<00:01, 160.16it/s]\u001b[A\n"," 15% 34/223 [00:00<00:01, 152.03it/s]\u001b[A\n"," 22% 50/223 [00:00<00:01, 151.37it/s]\u001b[A\n"," 30% 66/223 [00:00<00:01, 151.16it/s]\u001b[A\n"," 37% 82/223 [00:00<00:00, 152.98it/s]\u001b[A\n"," 44% 98/223 [00:00<00:00, 149.04it/s]\u001b[A\n"," 51% 114/223 [00:00<00:00, 149.77it/s]\u001b[A\n"," 58% 129/223 [00:00<00:00, 146.46it/s]\u001b[A\n"," 65% 144/223 [00:00<00:00, 147.14it/s]\u001b[A\n"," 71% 159/223 [00:01<00:00, 147.16it/s]\u001b[A\n"," 78% 174/223 [00:01<00:00, 146.64it/s]\u001b[A\n"," 85% 189/223 [00:01<00:00, 146.08it/s]\u001b[A\n"," 91% 204/223 [00:01<00:00, 146.12it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.3572661578655243, 'eval_LOC_precision': 0.592447278077489, 'eval_LOC_recall': 0.6003976143141153, 'eval_LOC_f1': 0.5963959516168849, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.25, 'eval_OBJ_recall': 0.0017574692442882249, 'eval_OBJ_f1': 0.0034904013961605585, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.392018779342723, 'eval_ORG_recall': 0.18191721132897604, 'eval_ORG_f1': 0.2485119047619048, 'eval_ORG_number': 918, 'eval_PER_precision': 0.5925309229305423, 'eval_PER_recall': 0.639866426920113, 'eval_PER_f1': 0.6152896134370753, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.5794994754982766, 'eval_micro_avg_recall': 0.5231331168831169, 'eval_micro_avg_f1': 0.54987557767508, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.45674924508768855, 'eval_macro_avg_recall': 0.35598468045187315, 'eval_macro_avg_f1': 0.36592196780300634, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.5412405365120575, 'eval_weighted_avg_recall': 0.5231331168831169, 'eval_weighted_avg_f1': 0.5175042054558375, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.883752906682843, 'eval_runtime': 2.245, 'eval_samples_per_second': 1587.951, 'eval_steps_per_second': 99.33, 'epoch': 1.0}\n"," 10% 2005/20050 [01:09<10:03, 29.92it/s]\n","100% 223/223 [00:02<00:00, 146.39it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:47:15,971 >> Saving model checkpoint to outputs/checkpoint-2005\n","[INFO|configuration_utils.py:446] 2022-05-14 22:47:15,973 >> Configuration saved in outputs/checkpoint-2005/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:47:16,087 >> Model weights saved in outputs/checkpoint-2005/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:47:16,087 >> tokenizer config file saved in outputs/checkpoint-2005/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:47:16,087 >> Special tokens file saved in outputs/checkpoint-2005/special_tokens_map.json\n","{'loss': 0.3034, 'learning_rate': 1.7777777777777777e-05, 'epoch': 2.0}\n"," 20% 4010/20050 [02:17<08:48, 30.38it/s][INFO|trainer.py:623] 2022-05-14 22:48:23,799 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:48:23,803 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:48:23,803 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:48:23,803 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  7% 15/223 [00:00<00:01, 148.58it/s]\u001b[A\n"," 13% 30/223 [00:00<00:01, 147.14it/s]\u001b[A\n"," 20% 45/223 [00:00<00:01, 144.21it/s]\u001b[A\n"," 27% 60/223 [00:00<00:01, 144.80it/s]\u001b[A\n"," 34% 75/223 [00:00<00:01, 144.84it/s]\u001b[A\n"," 40% 90/223 [00:00<00:00, 146.14it/s]\u001b[A\n"," 48% 106/223 [00:00<00:00, 147.59it/s]\u001b[A\n"," 54% 121/223 [00:00<00:00, 147.45it/s]\u001b[A\n"," 61% 137/223 [00:00<00:00, 149.18it/s]\u001b[A\n"," 68% 152/223 [00:01<00:00, 145.70it/s]\u001b[A\n"," 75% 167/223 [00:01<00:00, 144.47it/s]\u001b[A\n"," 82% 182/223 [00:01<00:00, 145.98it/s]\u001b[A\n"," 88% 197/223 [00:01<00:00, 140.72it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.298469215631485, 'eval_LOC_precision': 0.6052087639520463, 'eval_LOC_recall': 0.7276341948310139, 'eval_LOC_f1': 0.6607989167230873, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.48672566371681414, 'eval_OBJ_recall': 0.09666080843585237, 'eval_OBJ_f1': 0.16129032258064516, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.47130434782608693, 'eval_ORG_recall': 0.29520697167755994, 'eval_ORG_f1': 0.36302746148693904, 'eval_ORG_number': 918, 'eval_PER_precision': 0.635948706802869, 'eval_PER_recall': 0.7516054456717185, 'eval_PER_f1': 0.688956910760537, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6118318629994811, 'eval_micro_avg_recall': 0.637987012987013, 'eval_micro_avg_f1': 0.624635761589404, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5497968705744541, 'eval_macro_avg_recall': 0.46777685515403616, 'eval_macro_avg_f1': 0.4685184028878021, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.5956483553320212, 'eval_weighted_avg_recall': 0.637987012987013, 'eval_weighted_avg_f1': 0.600198874084283, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.8964917601860277, 'eval_runtime': 2.2643, 'eval_samples_per_second': 1574.442, 'eval_steps_per_second': 98.485, 'epoch': 2.0}\n"," 20% 4010/20050 [02:19<08:48, 30.38it/s]\n","100% 223/223 [00:02<00:00, 144.42it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:48:26,070 >> Saving model checkpoint to outputs/checkpoint-4010\n","[INFO|configuration_utils.py:446] 2022-05-14 22:48:26,071 >> Configuration saved in outputs/checkpoint-4010/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:48:26,168 >> Model weights saved in outputs/checkpoint-4010/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:48:26,168 >> tokenizer config file saved in outputs/checkpoint-4010/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:48:26,168 >> Special tokens file saved in outputs/checkpoint-4010/special_tokens_map.json\n","{'loss': 0.2497, 'learning_rate': 1.555555555555556e-05, 'epoch': 3.0}\n"," 30% 6015/20050 [03:27<07:50, 29.82it/s][INFO|trainer.py:623] 2022-05-14 22:49:34,058 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:49:34,061 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:49:34,061 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:49:34,061 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  7% 16/223 [00:00<00:01, 158.34it/s]\u001b[A\n"," 14% 32/223 [00:00<00:01, 147.02it/s]\u001b[A\n"," 22% 48/223 [00:00<00:01, 148.71it/s]\u001b[A\n"," 28% 63/223 [00:00<00:01, 148.51it/s]\u001b[A\n"," 35% 79/223 [00:00<00:00, 149.31it/s]\u001b[A\n"," 43% 95/223 [00:00<00:00, 150.13it/s]\u001b[A\n"," 50% 111/223 [00:00<00:00, 149.23it/s]\u001b[A\n"," 57% 127/223 [00:00<00:00, 150.30it/s]\u001b[A\n"," 64% 143/223 [00:00<00:00, 148.32it/s]\u001b[A\n"," 71% 158/223 [00:01<00:00, 148.35it/s]\u001b[A\n"," 78% 174/223 [00:01<00:00, 150.32it/s]\u001b[A\n"," 85% 190/223 [00:01<00:00, 145.45it/s]\u001b[A\n"," 92% 205/223 [00:01<00:00, 145.82it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.27784082293510437, 'eval_LOC_precision': 0.677891654465593, 'eval_LOC_recall': 0.6903578528827038, 'eval_LOC_f1': 0.6840679635557745, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.47648902821316613, 'eval_OBJ_recall': 0.2671353251318102, 'eval_OBJ_f1': 0.34234234234234234, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.45018007202881155, 'eval_ORG_recall': 0.4084967320261438, 'eval_ORG_f1': 0.4283266704740149, 'eval_ORG_number': 918, 'eval_PER_precision': 0.6721311475409836, 'eval_PER_recall': 0.7056254816337015, 'eval_PER_f1': 0.6884711779448622, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6398188803512623, 'eval_micro_avg_recall': 0.6308170995670995, 'eval_micro_avg_f1': 0.6352861035422342, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5691729755621385, 'eval_macro_avg_recall': 0.5179038479185898, 'eval_macro_avg_f1': 0.5358020385792486, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6310757750727223, 'eval_weighted_avg_recall': 0.6308170995670995, 'eval_weighted_avg_f1': 0.6283224722269352, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.9018501668183196, 'eval_runtime': 2.228, 'eval_samples_per_second': 1600.067, 'eval_steps_per_second': 100.088, 'epoch': 3.0}\n"," 30% 6015/20050 [03:29<07:50, 29.82it/s]\n","100% 223/223 [00:02<00:00, 148.43it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:49:36,292 >> Saving model checkpoint to outputs/checkpoint-6015\n","[INFO|configuration_utils.py:446] 2022-05-14 22:49:36,293 >> Configuration saved in outputs/checkpoint-6015/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:49:36,390 >> Model weights saved in outputs/checkpoint-6015/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:49:36,391 >> tokenizer config file saved in outputs/checkpoint-6015/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:49:36,391 >> Special tokens file saved in outputs/checkpoint-6015/special_tokens_map.json\n","{'loss': 0.2207, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}\n"," 40% 8020/20050 [04:37<06:40, 30.05it/s][INFO|trainer.py:623] 2022-05-14 22:50:44,492 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:50:44,495 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:50:44,495 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:50:44,495 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  7% 15/223 [00:00<00:01, 148.23it/s]\u001b[A\n"," 13% 30/223 [00:00<00:01, 148.67it/s]\u001b[A\n"," 20% 45/223 [00:00<00:01, 147.62it/s]\u001b[A\n"," 27% 61/223 [00:00<00:01, 148.84it/s]\u001b[A\n"," 35% 77/223 [00:00<00:00, 150.83it/s]\u001b[A\n"," 42% 93/223 [00:00<00:00, 149.53it/s]\u001b[A\n"," 48% 108/223 [00:00<00:00, 148.71it/s]\u001b[A\n"," 56% 124/223 [00:00<00:00, 149.39it/s]\u001b[A\n"," 62% 139/223 [00:00<00:00, 148.17it/s]\u001b[A\n"," 70% 155/223 [00:01<00:00, 148.98it/s]\u001b[A\n"," 76% 170/223 [00:01<00:00, 145.00it/s]\u001b[A\n"," 83% 185/223 [00:01<00:00, 145.15it/s]\u001b[A\n"," 90% 200/223 [00:01<00:00, 145.81it/s]\u001b[A\n","                                        \n","\u001b[A{'eval_loss': 0.27318528294563293, 'eval_LOC_precision': 0.6847557967439566, 'eval_LOC_recall': 0.6898608349900597, 'eval_LOC_f1': 0.6872988363456302, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.4837758112094395, 'eval_OBJ_recall': 0.28822495606326887, 'eval_OBJ_f1': 0.3612334801762115, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.48598130841121495, 'eval_ORG_recall': 0.39651416122004357, 'eval_ORG_f1': 0.43671265746850624, 'eval_ORG_number': 918, 'eval_PER_precision': 0.671973776633107, 'eval_PER_recall': 0.737220652453121, 'eval_PER_f1': 0.7030867221950025, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6479826699160574, 'eval_micro_avg_recall': 0.64745670995671, 'eval_micro_avg_f1': 0.6477195831641629, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5816216732494295, 'eval_macro_avg_recall': 0.5279551511816233, 'eval_macro_avg_f1': 0.5470829240463376, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6378682160688843, 'eval_weighted_avg_recall': 0.64745670995671, 'eval_weighted_avg_f1': 0.6393947426960099, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.9024769992922859, 'eval_runtime': 2.256, 'eval_samples_per_second': 1580.255, 'eval_steps_per_second': 98.849, 'epoch': 4.0}\n"," 40% 8020/20050 [04:40<06:40, 30.05it/s]\n","100% 223/223 [00:02<00:00, 145.68it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:50:46,754 >> Saving model checkpoint to outputs/checkpoint-8020\n","[INFO|configuration_utils.py:446] 2022-05-14 22:50:46,755 >> Configuration saved in outputs/checkpoint-8020/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:50:46,853 >> Model weights saved in outputs/checkpoint-8020/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:50:46,854 >> tokenizer config file saved in outputs/checkpoint-8020/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:50:46,854 >> Special tokens file saved in outputs/checkpoint-8020/special_tokens_map.json\n","{'loss': 0.1985, 'learning_rate': 1.1111111111111113e-05, 'epoch': 5.0}\n"," 50% 10025/20050 [05:48<05:30, 30.30it/s][INFO|trainer.py:623] 2022-05-14 22:51:54,905 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:51:54,907 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:51:54,907 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:51:54,908 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  7% 15/223 [00:00<00:01, 143.82it/s]\u001b[A\n"," 13% 30/223 [00:00<00:01, 146.78it/s]\u001b[A\n"," 21% 46/223 [00:00<00:01, 148.87it/s]\u001b[A\n"," 27% 61/223 [00:00<00:01, 145.31it/s]\u001b[A\n"," 34% 76/223 [00:00<00:01, 144.42it/s]\u001b[A\n"," 41% 91/223 [00:00<00:00, 145.53it/s]\u001b[A\n"," 48% 106/223 [00:00<00:00, 141.54it/s]\u001b[A\n"," 54% 121/223 [00:00<00:00, 140.92it/s]\u001b[A\n"," 61% 136/223 [00:00<00:00, 141.22it/s]\u001b[A\n"," 68% 151/223 [00:01<00:00, 138.56it/s]\u001b[A\n"," 74% 166/223 [00:01<00:00, 140.65it/s]\u001b[A\n"," 81% 181/223 [00:01<00:00, 140.34it/s]\u001b[A\n"," 88% 196/223 [00:01<00:00, 141.67it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 0.2787233591079712, 'eval_LOC_precision': 0.67611725132148, 'eval_LOC_recall': 0.6993041749502982, 'eval_LOC_f1': 0.687515269973125, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.4617169373549884, 'eval_OBJ_recall': 0.34973637961335674, 'eval_OBJ_f1': 0.398, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.435589519650655, 'eval_ORG_recall': 0.434640522875817, 'eval_ORG_f1': 0.43511450381679395, 'eval_ORG_number': 918, 'eval_PER_precision': 0.6416396979503776, 'eval_PER_recall': 0.7639352684305163, 'eval_PER_f1': 0.6974671669793621, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6175120922733474, 'eval_micro_avg_recall': 0.6735660173160173, 'eval_micro_avg_f1': 0.6443222258168877, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5537658515693753, 'eval_macro_avg_recall': 0.5619040864674971, 'eval_macro_avg_f1': 0.5545242351923202, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6115854126317543, 'eval_weighted_avg_recall': 0.6735660173160173, 'eval_weighted_avg_f1': 0.6391257465828464, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.8996865837630169, 'eval_runtime': 2.3064, 'eval_samples_per_second': 1545.668, 'eval_steps_per_second': 96.686, 'epoch': 5.0}\n"," 50% 10025/20050 [05:50<05:30, 30.30it/s]\n","100% 223/223 [00:02<00:00, 142.60it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:51:57,217 >> Saving model checkpoint to outputs/checkpoint-10025\n","[INFO|configuration_utils.py:446] 2022-05-14 22:51:57,218 >> Configuration saved in outputs/checkpoint-10025/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:51:57,316 >> Model weights saved in outputs/checkpoint-10025/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:51:57,316 >> tokenizer config file saved in outputs/checkpoint-10025/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:51:57,316 >> Special tokens file saved in outputs/checkpoint-10025/special_tokens_map.json\n","{'loss': 0.1798, 'learning_rate': 8.888888888888888e-06, 'epoch': 6.0}\n"," 60% 12030/20050 [06:58<04:26, 30.15it/s][INFO|trainer.py:623] 2022-05-14 22:53:05,026 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:53:05,029 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:53:05,029 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:53:05,029 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  8% 17/223 [00:00<00:01, 163.73it/s]\u001b[A\n"," 15% 34/223 [00:00<00:01, 154.13it/s]\u001b[A\n"," 22% 50/223 [00:00<00:01, 147.21it/s]\u001b[A\n"," 29% 65/223 [00:00<00:01, 137.61it/s]\u001b[A\n"," 36% 80/223 [00:00<00:01, 140.96it/s]\u001b[A\n"," 43% 95/223 [00:00<00:00, 143.35it/s]\u001b[A\n"," 50% 111/223 [00:00<00:00, 145.28it/s]\u001b[A\n"," 57% 126/223 [00:00<00:00, 143.45it/s]\u001b[A\n"," 64% 142/223 [00:00<00:00, 145.91it/s]\u001b[A\n"," 70% 157/223 [00:01<00:00, 145.07it/s]\u001b[A\n"," 77% 172/223 [00:01<00:00, 145.71it/s]\u001b[A\n"," 84% 187/223 [00:01<00:00, 144.40it/s]\u001b[A\n"," 91% 202/223 [00:01<00:00, 145.07it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 0.2847427725791931, 'eval_LOC_precision': 0.707936507936508, 'eval_LOC_recall': 0.6650099403578529, 'eval_LOC_f1': 0.6858021527421835, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.4864864864864865, 'eval_OBJ_recall': 0.2530755711775044, 'eval_OBJ_f1': 0.3329479768786127, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.46143617021276595, 'eval_ORG_recall': 0.3779956427015251, 'eval_ORG_f1': 0.41556886227544915, 'eval_ORG_number': 918, 'eval_PER_precision': 0.6714217784117926, 'eval_PER_recall': 0.725404572309273, 'eval_PER_f1': 0.6973700456846523, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6513157894736842, 'eval_micro_avg_recall': 0.6294642857142857, 'eval_micro_avg_f1': 0.6402036323610346, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5818202357618882, 'eval_macro_avg_recall': 0.5053714316365389, 'eval_macro_avg_f1': 0.5329222593952244, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6410474096849963, 'eval_weighted_avg_recall': 0.6294642857142857, 'eval_weighted_avg_f1': 0.63117358408826, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.9022950156708118, 'eval_runtime': 2.2538, 'eval_samples_per_second': 1581.788, 'eval_steps_per_second': 98.945, 'epoch': 6.0}\n"," 60% 12030/20050 [07:00<04:26, 30.15it/s]\n","100% 223/223 [00:02<00:00, 148.07it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:53:07,286 >> Saving model checkpoint to outputs/checkpoint-12030\n","[INFO|configuration_utils.py:446] 2022-05-14 22:53:07,287 >> Configuration saved in outputs/checkpoint-12030/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:53:07,392 >> Model weights saved in outputs/checkpoint-12030/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:53:07,392 >> tokenizer config file saved in outputs/checkpoint-12030/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:53:07,392 >> Special tokens file saved in outputs/checkpoint-12030/special_tokens_map.json\n","{'loss': 0.1639, 'learning_rate': 6.666666666666667e-06, 'epoch': 7.0}\n"," 70% 14035/20050 [08:08<03:19, 30.10it/s][INFO|trainer.py:623] 2022-05-14 22:54:15,329 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:54:15,332 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:54:15,332 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:54:15,332 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  8% 17/223 [00:00<00:01, 161.16it/s]\u001b[A\n"," 15% 34/223 [00:00<00:01, 149.83it/s]\u001b[A\n"," 22% 50/223 [00:00<00:01, 144.67it/s]\u001b[A\n"," 29% 65/223 [00:00<00:01, 146.34it/s]\u001b[A\n"," 36% 80/223 [00:00<00:00, 144.49it/s]\u001b[A\n"," 43% 96/223 [00:00<00:00, 147.00it/s]\u001b[A\n"," 50% 111/223 [00:00<00:00, 146.88it/s]\u001b[A\n"," 57% 126/223 [00:00<00:00, 146.00it/s]\u001b[A\n"," 63% 141/223 [00:00<00:00, 146.58it/s]\u001b[A\n"," 70% 156/223 [00:01<00:00, 146.39it/s]\u001b[A\n"," 77% 172/223 [00:01<00:00, 149.01it/s]\u001b[A\n"," 84% 187/223 [00:01<00:00, 149.10it/s]\u001b[A\n"," 91% 202/223 [00:01<00:00, 147.75it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 0.29092302918434143, 'eval_LOC_precision': 0.6953846153846154, 'eval_LOC_recall': 0.6739562624254473, 'eval_LOC_f1': 0.6845027763755679, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.4818181818181818, 'eval_OBJ_recall': 0.27943760984182775, 'eval_OBJ_f1': 0.353726362625139, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.42017738359201773, 'eval_ORG_recall': 0.4128540305010893, 'eval_ORG_f1': 0.4164835164835165, 'eval_ORG_number': 918, 'eval_PER_precision': 0.6775460122699386, 'eval_PER_recall': 0.7092216799383508, 'eval_PER_f1': 0.6930220883534136, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6414496348353314, 'eval_micro_avg_recall': 0.6297348484848485, 'eval_micro_avg_f1': 0.6355382619974059, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5687315482661883, 'eval_macro_avg_recall': 0.5188673956766787, 'eval_macro_avg_f1': 0.5369336859594093, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6353730865141687, 'eval_weighted_avg_recall': 0.6297348484848485, 'eval_weighted_avg_f1': 0.630243066084017, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.9012435547467395, 'eval_runtime': 2.3892, 'eval_samples_per_second': 1492.124, 'eval_steps_per_second': 93.336, 'epoch': 7.0}\n"," 70% 14035/20050 [08:11<03:19, 30.10it/s]\n","100% 223/223 [00:02<00:00, 146.93it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:54:17,724 >> Saving model checkpoint to outputs/checkpoint-14035\n","[INFO|configuration_utils.py:446] 2022-05-14 22:54:17,725 >> Configuration saved in outputs/checkpoint-14035/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:54:17,822 >> Model weights saved in outputs/checkpoint-14035/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:54:17,822 >> tokenizer config file saved in outputs/checkpoint-14035/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:54:17,822 >> Special tokens file saved in outputs/checkpoint-14035/special_tokens_map.json\n","{'loss': 0.151, 'learning_rate': 4.444444444444444e-06, 'epoch': 8.0}\n"," 80% 16040/20050 [09:18<02:16, 29.44it/s][INFO|trainer.py:623] 2022-05-14 22:55:25,628 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:55:25,631 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:55:25,631 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:55:25,632 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  8% 17/223 [00:00<00:01, 163.23it/s]\u001b[A\n"," 15% 34/223 [00:00<00:01, 154.20it/s]\u001b[A\n"," 22% 50/223 [00:00<00:01, 145.82it/s]\u001b[A\n"," 29% 65/223 [00:00<00:01, 142.98it/s]\u001b[A\n"," 36% 80/223 [00:00<00:00, 144.08it/s]\u001b[A\n"," 43% 95/223 [00:00<00:00, 144.23it/s]\u001b[A\n"," 49% 110/223 [00:00<00:00, 145.70it/s]\u001b[A\n"," 56% 125/223 [00:00<00:00, 143.33it/s]\u001b[A\n"," 63% 140/223 [00:00<00:00, 143.55it/s]\u001b[A\n"," 70% 155/223 [00:01<00:00, 144.32it/s]\u001b[A\n"," 76% 170/223 [00:01<00:00, 143.78it/s]\u001b[A\n"," 83% 185/223 [00:01<00:00, 145.47it/s]\u001b[A\n"," 90% 200/223 [00:01<00:00, 140.58it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 0.303473562002182, 'eval_LOC_precision': 0.6985446985446986, 'eval_LOC_recall': 0.6679920477137177, 'eval_LOC_f1': 0.6829268292682926, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.46153846153846156, 'eval_OBJ_recall': 0.26362038664323373, 'eval_OBJ_f1': 0.33557046979865773, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.4459295261239368, 'eval_ORG_recall': 0.3997821350762527, 'eval_ORG_f1': 0.4215967834577829, 'eval_ORG_number': 918, 'eval_PER_precision': 0.6904884318766067, 'eval_PER_recall': 0.6899563318777293, 'eval_PER_f1': 0.6902222793267377, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6531169204251652, 'eval_micro_avg_recall': 0.6151244588744589, 'eval_micro_avg_f1': 0.6335516232409085, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.574125279520926, 'eval_macro_avg_recall': 0.5053377253277334, 'eval_macro_avg_f1': 0.5325790904628678, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6446864297030197, 'eval_weighted_avg_recall': 0.6151244588744589, 'eval_weighted_avg_f1': 0.6275770506542852, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.9010615711252654, 'eval_runtime': 2.2736, 'eval_samples_per_second': 1568.008, 'eval_steps_per_second': 98.083, 'epoch': 8.0}\n"," 80% 16040/20050 [09:21<02:16, 29.44it/s]\n","100% 223/223 [00:02<00:00, 142.90it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:55:27,908 >> Saving model checkpoint to outputs/checkpoint-16040\n","[INFO|configuration_utils.py:446] 2022-05-14 22:55:27,909 >> Configuration saved in outputs/checkpoint-16040/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:55:28,011 >> Model weights saved in outputs/checkpoint-16040/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:55:28,011 >> tokenizer config file saved in outputs/checkpoint-16040/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:55:28,011 >> Special tokens file saved in outputs/checkpoint-16040/special_tokens_map.json\n","{'loss': 0.1407, 'learning_rate': 2.222222222222222e-06, 'epoch': 9.0}\n"," 90% 18045/20050 [10:30<01:07, 29.69it/s][INFO|trainer.py:623] 2022-05-14 22:56:36,870 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:56:36,872 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:56:36,872 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:56:36,873 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  7% 16/223 [00:00<00:01, 153.91it/s]\u001b[A\n"," 14% 32/223 [00:00<00:01, 150.84it/s]\u001b[A\n"," 22% 48/223 [00:00<00:01, 149.56it/s]\u001b[A\n"," 28% 63/223 [00:00<00:01, 144.05it/s]\u001b[A\n"," 35% 78/223 [00:00<00:00, 145.44it/s]\u001b[A\n"," 42% 94/223 [00:00<00:00, 148.02it/s]\u001b[A\n"," 49% 109/223 [00:00<00:00, 146.03it/s]\u001b[A\n"," 56% 124/223 [00:00<00:00, 145.89it/s]\u001b[A\n"," 63% 140/223 [00:00<00:00, 147.83it/s]\u001b[A\n"," 70% 155/223 [00:01<00:00, 144.41it/s]\u001b[A\n"," 77% 171/223 [00:01<00:00, 146.74it/s]\u001b[A\n"," 84% 187/223 [00:01<00:00, 148.28it/s]\u001b[A\n"," 91% 202/223 [00:01<00:00, 146.71it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 0.30953165888786316, 'eval_LOC_precision': 0.6814115308151093, 'eval_LOC_recall': 0.6814115308151093, 'eval_LOC_f1': 0.6814115308151093, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.4520884520884521, 'eval_OBJ_recall': 0.3233743409490334, 'eval_OBJ_f1': 0.3770491803278688, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.4391727493917275, 'eval_ORG_recall': 0.39324618736383443, 'eval_ORG_f1': 0.4149425287356322, 'eval_ORG_number': 918, 'eval_PER_precision': 0.6763020184400698, 'eval_PER_recall': 0.697148728487028, 'eval_PER_f1': 0.6865671641791045, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.638268541494348, 'eval_micro_avg_recall': 0.6263528138528138, 'eval_micro_avg_f1': 0.6322545404888706, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5622436876838397, 'eval_macro_avg_recall': 0.5237951969037513, 'eval_macro_avg_f1': 0.5399926010144287, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6309852098169814, 'eval_weighted_avg_recall': 0.6263528138528138, 'eval_weighted_avg_f1': 0.6276060870042102, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.899949448994035, 'eval_runtime': 2.2539, 'eval_samples_per_second': 1581.726, 'eval_steps_per_second': 98.941, 'epoch': 9.0}\n"," 90% 18045/20050 [10:32<01:07, 29.69it/s]\n","100% 223/223 [00:02<00:00, 143.44it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:56:39,130 >> Saving model checkpoint to outputs/checkpoint-18045\n","[INFO|configuration_utils.py:446] 2022-05-14 22:56:39,131 >> Configuration saved in outputs/checkpoint-18045/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:56:39,228 >> Model weights saved in outputs/checkpoint-18045/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:56:39,228 >> tokenizer config file saved in outputs/checkpoint-18045/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:56:39,229 >> Special tokens file saved in outputs/checkpoint-18045/special_tokens_map.json\n","{'loss': 0.1336, 'learning_rate': 0.0, 'epoch': 10.0}\n","100% 20050/20050 [11:40<00:00, 29.48it/s][INFO|trainer.py:623] 2022-05-14 22:57:47,075 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:57:47,077 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:57:47,077 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:57:47,077 >>   Batch size = 16\n","\n","  0% 0/223 [00:00<?, ?it/s]\u001b[A\n","  8% 17/223 [00:00<00:01, 160.24it/s]\u001b[A\n"," 15% 34/223 [00:00<00:01, 139.87it/s]\u001b[A\n"," 22% 50/223 [00:00<00:01, 145.03it/s]\u001b[A\n"," 29% 65/223 [00:00<00:01, 142.68it/s]\u001b[A\n"," 36% 80/223 [00:00<00:00, 143.39it/s]\u001b[A\n"," 43% 95/223 [00:00<00:00, 142.58it/s]\u001b[A\n"," 49% 110/223 [00:00<00:00, 144.34it/s]\u001b[A\n"," 56% 125/223 [00:00<00:00, 139.86it/s]\u001b[A\n"," 63% 140/223 [00:00<00:00, 140.05it/s]\u001b[A\n"," 70% 155/223 [00:01<00:00, 138.56it/s]\u001b[A\n"," 76% 169/223 [00:01<00:00, 138.10it/s]\u001b[A\n"," 82% 183/223 [00:01<00:00, 138.20it/s]\u001b[A\n"," 89% 198/223 [00:01<00:00, 141.53it/s]\u001b[A\n","                                         \n","\u001b[A{'eval_loss': 0.3112102746963501, 'eval_LOC_precision': 0.6827309236947792, 'eval_LOC_recall': 0.6759443339960238, 'eval_LOC_f1': 0.6793206793206794, 'eval_LOC_number': 2012, 'eval_OBJ_precision': 0.4547677261613692, 'eval_OBJ_recall': 0.3268892794376098, 'eval_OBJ_f1': 0.38036809815950917, 'eval_OBJ_number': 569, 'eval_ORG_precision': 0.41762013729977115, 'eval_ORG_recall': 0.39760348583877997, 'eval_ORG_f1': 0.4073660714285714, 'eval_ORG_number': 918, 'eval_PER_precision': 0.66943359375, 'eval_PER_recall': 0.7043411250963267, 'eval_PER_f1': 0.6864438603079234, 'eval_PER_number': 3893, 'eval_micro_avg_precision': 0.6312576312576312, 'eval_micro_avg_recall': 0.6294642857142857, 'eval_micro_avg_f1': 0.630359682991262, 'eval_micro_avg_number': 7392, 'eval_macro_avg_precision': 0.5561380952264798, 'eval_macro_avg_recall': 0.526194556092185, 'eval_macro_avg_f1': 0.5383746773041709, 'eval_macro_avg_number': 7392, 'eval_weighted_avg_precision': 0.6252567263487087, 'eval_weighted_avg_recall': 0.6294642857142857, 'eval_weighted_avg_f1': 0.6262866147722053, 'eval_weighted_avg_number': 7392, 'eval_overall_accuracy': 0.8990395308866647, 'eval_runtime': 2.3508, 'eval_samples_per_second': 1516.534, 'eval_steps_per_second': 94.863, 'epoch': 10.0}\n","100% 20050/20050 [11:42<00:00, 29.48it/s]\n","100% 223/223 [00:02<00:00, 141.23it/s]\u001b[A\n","                                      \u001b[A[INFO|trainer.py:2340] 2022-05-14 22:57:49,431 >> Saving model checkpoint to outputs/checkpoint-20050\n","[INFO|configuration_utils.py:446] 2022-05-14 22:57:49,432 >> Configuration saved in outputs/checkpoint-20050/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:57:49,534 >> Model weights saved in outputs/checkpoint-20050/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:57:49,535 >> tokenizer config file saved in outputs/checkpoint-20050/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:57:49,535 >> Special tokens file saved in outputs/checkpoint-20050/special_tokens_map.json\n","[INFO|trainer.py:1662] 2022-05-14 22:57:49,940 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 703.3165, 'train_samples_per_second': 912.178, 'train_steps_per_second': 28.508, 'train_loss': 0.2442883361664199, 'epoch': 10.0}\n","100% 20050/20050 [11:43<00:00, 28.51it/s]\n","[INFO|trainer.py:2340] 2022-05-14 22:57:49,942 >> Saving model checkpoint to outputs/\n","[INFO|configuration_utils.py:446] 2022-05-14 22:57:49,943 >> Configuration saved in outputs/config.json\n","[INFO|modeling_utils.py:1542] 2022-05-14 22:57:50,093 >> Model weights saved in outputs/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2108] 2022-05-14 22:57:50,093 >> tokenizer config file saved in outputs/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2114] 2022-05-14 22:57:50,094 >> Special tokens file saved in outputs/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =       10.0\n","  train_loss               =     0.2443\n","  train_runtime            = 0:11:43.31\n","  train_samples            =      64155\n","  train_samples_per_second =    912.178\n","  train_steps_per_second   =     28.508\n","05/14/2022 22:57:50 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:623] 2022-05-14 22:57:50,161 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2590] 2022-05-14 22:57:50,164 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2592] 2022-05-14 22:57:50,164 >>   Num examples = 3565\n","[INFO|trainer.py:2595] 2022-05-14 22:57:50,164 >>   Batch size = 16\n","100% 223/223 [00:02<00:00, 96.22it/s] \n","***** eval metrics *****\n","  epoch                       =       10.0\n","  eval_LOC_f1                 =     0.6793\n","  eval_LOC_number             =       2012\n","  eval_LOC_precision          =     0.6827\n","  eval_LOC_recall             =     0.6759\n","  eval_OBJ_f1                 =     0.3804\n","  eval_OBJ_number             =        569\n","  eval_OBJ_precision          =     0.4548\n","  eval_OBJ_recall             =     0.3269\n","  eval_ORG_f1                 =     0.4074\n","  eval_ORG_number             =        918\n","  eval_ORG_precision          =     0.4176\n","  eval_ORG_recall             =     0.3976\n","  eval_PER_f1                 =     0.6864\n","  eval_PER_number             =       3893\n","  eval_PER_precision          =     0.6694\n","  eval_PER_recall             =     0.7043\n","  eval_loss                   =     0.3112\n","  eval_macro_avg_f1           =     0.5384\n","  eval_macro_avg_number       =       7392\n","  eval_macro_avg_precision    =     0.5561\n","  eval_macro_avg_recall       =     0.5262\n","  eval_micro_avg_f1           =     0.6304\n","  eval_micro_avg_number       =       7392\n","  eval_micro_avg_precision    =     0.6313\n","  eval_micro_avg_recall       =     0.6295\n","  eval_overall_accuracy       =      0.899\n","  eval_runtime                = 0:00:02.32\n","  eval_samples                =       3565\n","  eval_samples_per_second     =     1532.3\n","  eval_steps_per_second       =     95.849\n","  eval_weighted_avg_f1        =     0.6263\n","  eval_weighted_avg_number    =       7392\n","  eval_weighted_avg_precision =     0.6253\n","  eval_weighted_avg_recall    =     0.6295\n"]}]},{"cell_type":"code","source":["!rm -rf /content/banglabert"],"metadata":{"id":"AJeSF61I27ft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DH7ORXkHg_iX"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"NER.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}